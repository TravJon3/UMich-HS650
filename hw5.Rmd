---
title: "Homework 5"
subtitle: "<h2><u>Data Science and Predictive Analytics (HS650), Fall 2021</u></h2>"
author: "<h3>Kevin Wu</h3>"
date: "`r format(Sys.time(), '%B %Y')`"
tags: [DSPA, SOCR, MIDAS, Big Data, Predictive Analytics] 
output:
  html_document:
    theme: spacelab
    highlight: tango
    toc: true
    number_sections: true
    toc_depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
    code_folding: show
    self_contained: yes
---
**Homework_5_HS650_Fall_2021.Rmd**.

 * HW #5
 * Fall 2021, DSPA (HS650)
 * Name: Kevin Wu
 * SID: ####0012 (last 4 digits only)
 * UMich E-mail: kevinkwu@umich.edu
 * I certify that the following paper represents my own independent work and conforms with the guidelines of academic honesty described in the UMich student handbook.
 * Remember that students are allowed and encouraged to discuss, on a conceptual level, the problems with your class mates, however, this can not involve the exchange of actual code, printouts, solutions, e-mails or other explicit electronic or paper handouts.

```{r Libararies, warning=F, message=F}
library(plotly)
library(neuralnet)
library(DT)
library(stats)
library(cluster)
library(matrixStats)
library(cluster)
library(mclust)
```

# HW Problem 5.1, Learning the Power Law

Design, train, and optimize a generic neural network (NN) that can learn and predict the power-function (Links to an external site.) for a given power parameter (λ∈R). Assess the accuracy of the NN prediction of the power function. [Hint: Recall the example with the square-root function (Links to an external site.).]
```{r 5.1, warning=F, message=F}

# generate random training data: 100 random data points randomly uniformnly distributed between -5 and 5 and power function of either -1, 2 or 3... any other power value does not seem to converge
rand_base <- runif(100, -5, 5)
set.seed(123)
rand_power <- sample(x=c(-1,2,3), size=1)

# create a 2 column data-frame (rand_base_data, power_data)
power_df <- data.frame(rand_base_data=rand_base, power_data=rand_base^rand_power) 
# plot(power_df$rand_base_data, power_df$power_data)

plot_ly(power_df, x = ~rand_base_data, y = ~power_data, type="scatter", mode="markers") %>%
  layout(title='Power Function',
           xaxis = list(title="Input (x)"),
           yaxis = list(title=paste0("Output (y=x^", rand_power ,")")),
           legend = list(orientation = 'h'))

set.seed(1234)
net_power <- neuralnet(power_data ~ rand_base_data, power_df, hidden=10, threshold=0.1)

plot(net_power, rep="best")

# generate testing data sequence (from=0.1, from -7 to 7 that goes 2 units beyond the training data).
test_base <- seq(-7, 7, 0.1)
test_power_data <- test_base^rand_power
test_data_df <- data.frame(rand_base_data=test_base, power_data=test_power_data)
pred_power <- predict(net_power, test_data_df)

# compares the predicted power function generated by the Neural Net Model with the actual power function
# it seems the neural net is underestimating the power function.
plot_ly(x = ~test_base, y = ~test_power_data,  type="scatter", mode="lines", name="power") %>%
  add_trace(x = ~test_base, y = ~pred_power, mode="markers", name="NN Model Prediction") %>%
  layout(title='Predicted Neural Net Power Function vs. Actual Power Function',
           xaxis = list(title="Inputs"),
           yaxis = list(title="Outputs (y=power(x))"),
           legend = list(orientation = 'h'))
```
# HW Problem 5.2 (ALS Clustering):

Use the ALS dataset to study a rare but devastating progressive neurodegenerative disease, amyotrophic lateral sclerosis (ALS). Major clinically relevant questions include: What patient phenotypes can be automatically and reliably identified and used to predict the change of the ALSFRS slope over time?
```{r 5.2, warning=F, message=F}

pathname_ALS_HW5 <- "http://kevinwurn.github.io/UMich-HS650/hw5_files/"
pathname_ALS_small <- paste0(pathname_ALS_HW5, "ALS_TestingData_78.csv")
pathname_ALS_big <- paste0(pathname_ALS_HW5, "ALS_TrainingData_2223.csv")
als_df <- read.csv(pathname_ALS_big)
als_df <- als_df[-1]

# preliminary data summaries and visualizations
datatable(als_df)
str(als_df)
plot_ly() %>%
  add_trace(x = ~als_df$hands_median, name="hands_median", type="box") %>%
  add_trace(x = ~als_df$leg_median, name="leg_median", type="box") %>%
    layout(title="Median Leg and Hands",
      xaxis=list(title="Medians"))

plot_ly() %>%
  add_trace(x = ~als_df$ALSFRS_Total_median, type="histogram") %>%
    layout(title="Histogram of Median ALSFRS Score",
      xaxis=list(title="ALSFRS Median Score"), bargap=0.1)

# started with sqrt(2200 / 2) which is roughly 33, however it is clear that k being 33 is suboptimal 
als_df_z <- as.data.frame(lapply(als_df, scale))

set_silhouette <- function(df, k) {
  set.seed(123)
  clusters <- kmeans(df, k)
  silhouette(clusters$cluster, dist(df))
}

sil <- set_silhouette(als_df_z, 33)
summary(sil)
# mean width being 0.03 k needs to be smaller.  let's optimize for k.
sil <- set_silhouette(als_df_z, 28)
summary(sil)
sil <- set_silhouette(als_df_z, 23)
summary(sil)
sil <- set_silhouette(als_df_z, 18)
summary(sil)
sil <- set_silhouette(als_df_z, 13)
summary(sil)
# with k <- 8 now we have a drop in the mean width from 0.040451 to 0.040341  
sil <- set_silhouette(als_df_z, 8)
summary(sil)
# with k <- 10 we have a better improvement 0.045682, better than k <- 8 and k <- 13.  k <- 10 seems good enough
sil <- set_silhouette(als_df_z, 10)
summary(sil)

# let's plot the silhouettes where k <- 10
set.seed(123)
als_clusters <- kmeans(als_df_z, 10)
dis <- dist(als_df_z)
sil <- silhouette(als_clusters$cluster, dis)
plot(sil, col=c(1:length(als_clusters$centers[ ,1])), border=NA)
factoextra::fviz_silhouette(sil, label=T, palette = "jco", ggtheme = theme_classic())

# Clusters 8 and 10 are the most homogeneous with average widths a little over 0.1.  not great.
# Cluster 8 - ALSFRS, respirations, hands, mouth, trunk, and leg median / min / max scores as previously shown as well as is the 2nd most average wide cluster with width 0.11... all related to ALS and muscle movement understandable.  Also understandable is creatinine which is related to muscle breakdown -  
# Cluster 10 - 0.14
df <- as.data.frame(t(als_clusters$centers))
rowNames <- rownames(df)
colnames(df) <- paste0("Cluster",c(1:10))
plot_ly(df, x = rownames(df), y = ~Cluster1, type = 'bar', name = 'Cluster1') %>%
  add_trace(y = ~Cluster2, name = 'Cluster2') %>%
  add_trace(y = ~Cluster3, name = 'Cluster3') %>%
  add_trace(y = ~Cluster4, name = 'Cluster4') %>%
  add_trace(y = ~Cluster5, name = 'Cluster5') %>%
  add_trace(y = ~Cluster6, name = 'Cluster6') %>%
  add_trace(y = ~Cluster7, name = 'Cluster7') %>%
  add_trace(y = ~Cluster8, name = 'Cluster8') %>%
  add_trace(y = ~Cluster9, name = 'Cluster9') %>%
  add_trace(y = ~Cluster10, name = 'Cluster10') %>%
  layout(title="Explicating Derived Cluster Labels",
         yaxis = list(title = 'Cluster Centers'), barmode = 'group')


kpp_init = function(dat, K) {
  x = as.matrix(dat)
  n = nrow(x)
  # Randomly choose a first center
  centers = matrix(NA, nrow=K, ncol=ncol(x))
  set.seed(123)
  centers[1,] = as.matrix(x[sample(1:n, 1),])
  for (k in 2:K) {
    # Calculate dist^2 to closest center for each point
    dists = matrix(NA, nrow=n, ncol=k-1)
    for (j in 1:(k-1)) {
      temp = sweep(x, 2, centers[j,], '-')
      dists[,j] = rowSums(temp^2)
    }
    dists = rowMins(dists)
    # Draw next center with probability proportional to dist^2
    cumdists = cumsum(dists)
    prop = runif(1, min=0, max=cumdists[n])
    centers[k,] = as.matrix(x[min(which(cumdists > prop)),])
  }
  return(centers)
}

set.seed(123)
n_rows <- 10
# it appears k-means++ appears to have done worse with the same number of k <- 10.  0.04 vs 0.05
clust_kpp = kmeans(als_df_z, kpp_init(als_df_z, n_rows), iter.max=100, algorithm='Lloyd')
sil2 = silhouette(clust_kpp$cluster, dis)
summary(sil2)
plot(sil2, col=c(1:length(clust_kpp$centers[ ,1])), border=NA)

# looks like the elbow point is k <- 6
mat = matrix(0, nrow = n_rows)
for (i in 2:n_rows){
  set.seed(123)
  clust_kpp = kmeans(als_df_z, kpp_init(als_df_z, i), iter.max=100, algorithm='Lloyd')
  sil = silhouette(clust_kpp$cluster, dis)
  mat[i] = mean(as.matrix(sil)[,3])
}
mat
df <- data.frame(k = 2:n_rows, sil = mat[2:n_rows])
plot_ly(df, x = ~k, y = ~sil, type = 'scatter', mode = 'lines', name='Silhouette') %>%
  layout(title="Average Silhouette Graph")


# appears to perform just as good as kmeans 0.05, but a smaller k which is better as it arranges the clusters in a more homogneously
K <- 6
set.seed(123)
clust_kpp2 = kmeans(als_df_z, kpp_init(als_df_z, K), iter.max=100, algorithm="MacQueen")
sil3 = silhouette(clust_kpp2$cluster, dis)
summary(sil3)
plot(sil3, col=c(1:length(clust_kpp2$centers[ ,1])), border=NA)


# explicated 6 clusters optimized - interpret the results!!!
#
#
df <- as.data.frame(t(clust_kpp$centers))
rowNames <- rownames(df)
colnames(df) <- paste0("Cluster",c(1:K))
plot_ly(df, x = rownames(df), y = ~Cluster1, type = 'bar', name = 'Cluster1') %>%
  add_trace(y = ~Cluster2, name = 'Cluster2') %>%
  add_trace(y = ~Cluster3, name = 'Cluster3') %>%
  add_trace(y = ~Cluster4, name = 'Cluster4') %>%
  add_trace(y = ~Cluster5, name = 'Cluster5') %>%
  add_trace(y = ~Cluster6, name = 'Cluster6') %>%
  layout(title="Explicating Derived Cluster Labels",
         yaxis = list(title = 'Cluster Centers'), barmode = 'group')

# as noted by the plots, single linkage with 0.57 is most successful... for some reason the graphs aren't plotting
als_sing = agnes(als_df_z, diss=FALSE, method='single')
als_comp = agnes(als_df_z, diss=FALSE, method='complete')
als_ward = agnes(als_df_z, diss=FALSE, method='ward')
sil_sing = silhouette(cutree(als_sing, k=K), dis)
sil_comp = silhouette(cutree(als_comp, k=K), dis)
sil_ward = silhouette(cutree(als_ward, k=9), dis)
plot(sil_sing, col=1:length(unique(sil_sing[,1])))
plot(sil_comp, col=1:length(unique(sil_comp[,1])))
plot(sil_ward, col=1:length(unique(sil_ward[,1])))
```
# HW Problem 5.2b (Gaussian mixture model):

Fit a Gaussian mixture model, select the optimal model, report BIC, and display density and classification plots
Compare the result of the above methods

```{r 5.2b, warning=F, message=F}
# Gaussioan mixture models
set.seed(123)
als_clinical_df <- als_df_z[c(6, 7, 8, 10, 24, 25, 26, 28, 32, 36, 40, 44, 49, 61, 65, 66, 67, 69, 72, 73, 90, 94, 95)]
str(als_clinical_df)
gmm_clust <- Mclust(als_clinical_df)

# Thus the optimal model name is EEV
gmm_clust$modelName
gmm_clust$BIC

# Below are the 3 components
plot(gmm_clust$BIC, legendArgs = list(x = "bottom", ncol = 2, cex = 1))
plot(gmm_clust, what = "density")
plot(gmm_clust, what = "classification")